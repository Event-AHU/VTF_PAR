<img src="https://github.com/Event-AHU/VTF_PAR/blob/main/figures/frameworkV4.jpg" width="600">


# VTF_PAR
> **Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition**, Jun Zhu†, Jiandong Jin†, Zihan Yang, Xiaohao Wu, Xiao Wang († denotes equal contribution), CVPR-2023 Workshop@NFVLR (New Frontiers in Visual Language Reasoning: Compositionality, Prompts and Causality),  
[[Paper]()] 
[[Workshop](https://nfvlr-workshop.github.io/)] 



## Datasets and Pre-trained Models 

**MARS Dataset**: 

**Pre-trained Models**: 链接：https://pan.baidu.com/s/150t_zCW35YQHViKxsRIVzQ  提取码：glbd 

## Training and Testing 




## :page_with_curl: BibTex: 
If you find this work useful for your research, please cite the following papers: 

```bibtex
@article{zhu2023videoPAR,
  title={Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition},
  author={Jun Zhu, Jiandong Jin, Zihan Yang, Xiaohao Wu, Xiao Wang},
  journal={arXiv},
  year={2023}
}
```

If you have any questions about this work, please submit an issue or contact me via **Email**: wangxiaocvpr@foxmail.com or xiaowang@ahu.edu.cn. Thanks for your attention! 





